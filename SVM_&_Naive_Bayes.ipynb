{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **SVM & Naive Bayes**\n",
        "Question 1: What is a Support Vector Machine (SVM), and how does it work?\n",
        "- ### Support Vector Machine (SVM)\n",
        "\n",
        "A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks. It aims to find the best hyperplane that separates the data into different classes.\n",
        "\n",
        "- ### How SVM Works\n",
        "1. Data Preparation: SVM takes labeled data as input and tries to find the best hyperplane that separates the classes.\n",
        "\n",
        "2. Hyperplane Selection: SVM selects the hyperplane that maximizes the margin between the classes. The margin is the distance between the hyperplane and the nearest data points (support vectors) of each class.\n",
        "\n",
        "3. Support Vectors: The data points that lie closest to the hyperplane are called support vectors. These points are crucial in defining the position and orientation of the hyperplane.\n",
        "4. Kernel Trick: SVM can use the kernel trick to handle non-linearly separable data. It maps the data into a higher-dimensional space where the data becomes linearly separable.\n",
        "\n",
        "- ### Types of SVM\n",
        "1. Linear SVM: Used for linearly separable data.\n",
        "\n",
        "2. Non-Linear SVM: Used for non-linearly separable data, often with the help of kernel functions like polynomial, radial basis function (RBF), or sigmoid.\n",
        "\n",
        "- ### Advantages of SVM\n",
        "1. Effective in High-Dimensional Spaces: SVM is effective in high-dimensional spaces, making it suitable for complex datasets.\n",
        "\n",
        "2. Robust to Noise: SVM is robust to noise and outliers, especially when using a suitable kernel function.\n",
        "\n",
        "3. Flexible: SVM can be used for both classification and regression tasks.\n",
        "\n",
        "- ### Applications of SVM\n",
        "1. Image Classification: SVM is widely used in image classification tasks, such as object detection and recognition.\n",
        "\n",
        "2. Text Classification: SVM is used in text classification tasks, such as spam detection and sentiment analysis.\n",
        "3. Bioinformatics: SVM is used in bioinformatics for tasks like protein classification and gene expression analysis.\n",
        "\n",
        "Overall, SVM is a powerful algorithm that can handle complex datasets and provide accurate results in various applications."
      ],
      "metadata": {
        "id": "93bww9IGE4mi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: Explain the difference between Hard Margin and Soft Margin SVM.\n",
        "- ## Hard Margin SVM vs Soft Margin SVM\n",
        "The main difference between Hard Margin SVM and Soft Margin SVM lies in how they handle the separation of classes.\n",
        "\n",
        "### Hard Margin SVM\n",
        "1. **Strict Separation:** Hard Margin SVM requires that the data points be linearly separable, meaning that the classes can be separated by a hyperplane without any misclassifications.\n",
        "2. **Maximum Margin:** The goal is to find the hyperplane that maximizes the margin between the classes.\n",
        "3. **No Slack Variables:** Hard Margin SVM does not allow for any slack variables, meaning that all data points must lie on the correct side of the hyperplane.\n",
        "\n",
        "### Soft Margin SVM\n",
        "1. **Allowance for Misclassifications:** Soft Margin SVM allows for some misclassifications by introducing slack variables, which permit data points to lie on the wrong side of the hyperplane.\n",
        "2. **Trade-off between Margin and Misclassifications:** Soft Margin SVM balances the need for a large margin with the need to minimize misclassifications.\n",
        "3. **Regularization Parameter:** The regularization parameter (C) controls the trade-off between the margin and misclassifications. A small value of C allows for more misclassifications, while a large value of C enforces a stricter separation.\n",
        "\n",
        "## Key Differences\n",
        "1. **Handling Non-Separable Data:** Soft Margin SVM can handle non-separable data, while Hard Margin SVM requires linearly separable data.\n",
        "2. **Robustness to Noise:** Soft Margin SVM is more robust to noise and outliers due to the allowance for misclassifications.\n",
        "3. **Flexibility:** Soft Margin SVM provides more flexibility in handling real-world datasets, which often contain noise and non-linear relationships.\n",
        "\n",
        "## Choosing between Hard Margin and Soft Margin SVM\n",
        "1. **Data Characteristics:** If the data is linearly separable and noise-free, Hard Margin SVM might be suitable. However, if the data is noisy or non-separable, Soft Margin SVM is a better choice.\n",
        "2. **Model Complexity:** Soft Margin SVM provides more flexibility and can handle complex datasets, but it may require careful tuning of the regularization parameter.\n",
        "\n",
        "By understanding the differences between Hard Margin and Soft Margin SVM, you can choose the most suitable approach for your specific classification task.\n",
        "\n",
        "Question 3: What is the Kernel Trick in SVM? Give one example of a kernel and\n",
        "explain its use case.\n",
        "- ## Kernel Trick in SVM\n",
        "The Kernel Trick is a technique used in Support Vector Machines (SVMs) to handle non-linearly separable data. It maps the original data into a higher-dimensional space where the data becomes linearly separable, without explicitly computing the coordinates of the data points in that space.\n",
        "\n",
        "## How Kernel Trick Works\n",
        "1. **Mapping Data:** The kernel function maps the original data into a higher-dimensional space, where the data becomes linearly separable.\n",
        "2. **Computing Dot Products:** The kernel function computes the dot product of the data points in the higher-dimensional space, without explicitly computing the coordinates of the data points.\n",
        "\n",
        "## Example of a Kernel: Radial Basis Function (RBF) Kernel\n",
        "The RBF kernel is a popular kernel function used in SVMs. It maps the data into an infinite-dimensional space and is defined as:\n",
        "\n",
        "K(x, y) = exp(-γ||x - y||^2)\n",
        "\n",
        "where γ is a hyperparameter that controls the width of the kernel.\n",
        "\n",
        "## Use Case of RBF Kernel\n",
        "The RBF kernel is useful when the data is non-linearly separable and has complex relationships between features. For example, in image classification tasks, the RBF kernel can help the SVM model capture non-linear patterns in the image data.\n",
        "\n",
        "## Advantages of Kernel Trick\n",
        "1. **Handling Non-Linear Data:** The kernel trick allows SVMs to handle non-linearly separable data, making it a powerful tool for complex classification tasks.\n",
        "2. **Flexibility:** The kernel trick provides flexibility in choosing the kernel function, allowing users to select the most suitable kernel for their specific problem.\n",
        "3. **Efficient Computation:** The kernel trick avoids the need to explicitly compute the coordinates of the data points in the higher-dimensional space, making it computationally efficient.\n",
        "\n",
        "By using the kernel trick, SVMs can effectively handle non-linearly separable data and provide accurate results in various applications.\n",
        "\n",
        "Question 4: What is a Naïve Bayes Classifier, and why is it called “naïve”?\n",
        "- ## Naïve Bayes Classifier\n",
        "A Naïve Bayes Classifier is a type of supervised machine learning algorithm used for classification tasks. It is based on Bayes' theorem and assumes independence between features.\n",
        "\n",
        "## How Naïve Bayes Works\n",
        "1. **Bayes' Theorem:** The algorithm uses Bayes' theorem to calculate the probability of a class given the input features.\n",
        "2. **Independence Assumption:** The \"naïve\" part of the algorithm comes from the assumption that the features are independent of each other, which is often not true in real-world datasets.\n",
        "3. **Calculating Probabilities:** The algorithm calculates the probability of each class given the input features and selects the class with the highest probability.\n",
        "\n",
        "## Why is it Called \"Naïve\"?\n",
        "The Naïve Bayes Classifier is called \"naïve\" because of the strong assumption it makes about the independence of features. In reality, features are often correlated or dependent on each other, which can lead to inaccurate predictions. Despite this assumption, the Naïve Bayes Classifier often performs well in practice, especially for text classification tasks.\n",
        "\n",
        "## Advantages of Naïve Bayes\n",
        "1. **Simple and Efficient:** The Naïve Bayes Classifier is a simple and efficient algorithm that can handle large datasets.\n",
        "2. **Easy to Implement:** The algorithm is easy to implement and requires minimal tuning of hyperparameters.\n",
        "3. **Good for Text Classification:** Naïve Bayes is often used for text classification tasks, such as spam detection and sentiment analysis.\n",
        "\n",
        "## Use Cases\n",
        "1. **Text Classification:** Naïve Bayes is widely used for text classification tasks, such as spam detection, sentiment analysis, and topic modeling.\n",
        "2. **Document Classification:** The algorithm can be used for document classification tasks, such as classifying news articles or scientific papers.\n",
        "3. **Recommendation Systems:** Naïve Bayes can be used in recommendation systems to predict user preferences based on their past behavior.\n",
        "\n",
        "Overall, the Naïve Bayes Classifier is a simple yet effective algorithm that can provide good results in various classification tasks, despite its strong assumption about feature independence.\n",
        "\n",
        "Question 5: Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants. When would you use each one?\n",
        "- ## Naïve Bayes Variants\n",
        "There are several variants of the Naïve Bayes algorithm, each suited for different types of data and applications. Here are three common variants:\n",
        "\n",
        "### Gaussian Naïve Bayes\n",
        "1. **Assumes Gaussian Distribution:** Gaussian Naïve Bayes assumes that the features follow a Gaussian distribution (normal distribution).\n",
        "2. **Continuous Data:** This variant is suitable for continuous data, such as numerical features.\n",
        "3. **Use Cases:** Gaussian Naïve Bayes is often used in applications like regression tasks, predicting continuous outcomes, and handling datasets with continuous features.\n",
        "\n",
        "### Multinomial Naïve Bayes\n",
        "1. **Discrete Counts:** Multinomial Naïve Bayes is suitable for discrete counts, such as word frequencies in text data.\n",
        "2. **Text Classification:** This variant is often used for text classification tasks, such as spam detection, sentiment analysis, and topic modeling.\n",
        "3. **Multiple Features:** Multinomial Naïve Bayes can handle multiple features with different numbers of possible values.\n",
        "\n",
        "### Bernoulli Naïve Bayes\n",
        "1. **Binary Features:** Bernoulli Naïve Bayes is suitable for binary features, such as presence or absence of a word in a document.\n",
        "2. **Text Classification:** This variant is often used for text classification tasks, especially when the features are binary.\n",
        "3. **Simple and Efficient:** Bernoulli Naïve Bayes is a simple and efficient algorithm that can provide good results for binary feature datasets.\n",
        "\n",
        "## Choosing the Right Variant\n",
        "1. **Data Type:** Choose the variant based on the type of data you have:\n",
        "    - Gaussian Naïve Bayes for continuous data.\n",
        "    - Multinomial Naïve Bayes for discrete counts.\n",
        "    - Bernoulli Naïve Bayes for binary features.\n",
        "2. **Application:** Consider the specific application and the characteristics of the data:\n",
        "    - Text classification: Multinomial or Bernoulli Naïve Bayes.\n",
        "    - Regression tasks: Gaussian Naïve Bayes.\n",
        "\n",
        "By selecting the right Naïve Bayes variant for your specific problem, you can improve the accuracy and efficiency of your classification model.\n",
        "\n",
        "## Dataset Info:\n",
        "\n",
        "● You can use any suitable datasets like Iris, Breast Cancer, or Wine from\n",
        "sklearn.datasets or a CSV file you have.\n",
        "\n",
        "Question 6: Write a Python program to:\n",
        "\n",
        "● Load the Iris dataset\n",
        "\n",
        "● Train an SVM Classifier with a linear kernel\n",
        "\n",
        "● Print the model's accuracy and support vectors.\n"
      ],
      "metadata": {
        "id": "T5NXRRIvGE7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM Classifier with Linear Kernel on Iris Dataset\n",
        "# Here's a Python program that loads the Iris dataset, trains an SVM classifier with a linear kernel, and prints the model's accuracy and support vectors.\n",
        "\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train an SVM classifier with a linear kernel\n",
        "svm_classifier = svm.SVC(kernel='linear')\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the test set\n",
        "y_pred = svm_classifier.predict(X_test)\n",
        "\n",
        "# Calculate and print the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n",
        "# Print the support vectors\n",
        "print(\"Support Vectors:\")\n",
        "print(svm_classifier.support_vectors_)\n",
        "\n",
        "\n",
        "# Explanation\n",
        "# 1. Loading the Iris Dataset: The program loads the Iris dataset using sklearn.datasets.load_iris().\n",
        "# 2. Splitting the Dataset: The dataset is split into training and testing sets using train_test_split().\n",
        "# 3. Training the SVM Classifier: An SVM classifier with a linear kernel is trained using svm.SVC(kernel='linear').\n",
        "# 4. Predicting Labels: The trained model predicts the labels for the test set using predict().\n",
        "# 5. Calculating Accuracy: The model's accuracy is calculated using accuracy_score() and printed.\n",
        "# 6. Printing Support Vectors: The support vectors are printed using svm_classifier.support_vectors_.\n",
        "\n",
        "# This program demonstrates how to use an SVM classifier with a linear kernel on the Iris dataset and evaluate its performance."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vi2pGx2nKx2A",
        "outputId": "9ebd9828-c6bd-467f-99a9-e66f3974ea1e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "Support Vectors:\n",
            "[[4.8 3.4 1.9 0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.7 3.  5.  1.7]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [6.3 2.5 5.  1.9]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [5.9 3.  5.1 1.8]\n",
            " [4.9 2.5 4.5 1.7]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to:\n",
        "\n",
        "● Load the Breast Cancer dataset\n",
        "\n",
        "● Train a Gaussian Naïve Bayes model\n",
        "\n",
        "● Print its classification report including precision, recall, and F1-score.\n"
      ],
      "metadata": {
        "id": "Xc3Z9CSSLH1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gaussian Naïve Bayes Model on Breast Cancer Dataset\n",
        "# Here's a Python program that loads the Breast Cancer dataset, trains a Gaussian Naïve Bayes model, and prints its classification report.\n",
        "\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "breast_cancer = datasets.load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Gaussian Naïve Bayes model\n",
        "gnb_model = GaussianNB()\n",
        "gnb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the test set\n",
        "y_pred = gnb_model.predict(X_test)\n",
        "\n",
        "# Print the classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "# Explanation\n",
        "# 1. Loading the Breast Cancer Dataset: The program loads the Breast Cancer dataset using sklearn.datasets.load_breast_cancer().\n",
        "# 2. Splitting the Dataset: The dataset is split into training and testing sets using train_test_split().\n",
        "# 3. Training the Gaussian Naïve Bayes Model: A Gaussian Naïve Bayes model is trained using GaussianNB().\n",
        "# 4. Predicting Labels: The trained model predicts the labels for the test set using predict().\n",
        "# 5. Printing Classification Report: The classification report is printed using classification_report(), which includes precision, recall, and F1-score for each class.\n",
        "\n",
        "# This program demonstrates how to use a Gaussian Naïve Bayes model on the Breast Cancer dataset and evaluate its performance using a classification report."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niIHX7EDLbvY",
        "outputId": "dad1b84b-7811-4501-df48-05e7780be26f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.93      0.96        43\n",
            "           1       0.96      1.00      0.98        71\n",
            "\n",
            "    accuracy                           0.97       114\n",
            "   macro avg       0.98      0.97      0.97       114\n",
            "weighted avg       0.97      0.97      0.97       114\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "\n",
        "● Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best\n",
        "C and gamma.\n",
        "\n",
        "● Print the best hyperparameters and accuracy.\n"
      ],
      "metadata": {
        "id": "V0GrcaFNLsOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM Classifier with GridSearchCV on Wine Dataset\n",
        "# Here's a Python program that trains an SVM classifier on the Wine dataset using GridSearchCV to find the best hyperparameters (C and gamma) and prints the best hyperparameters and accuracy.\n",
        "\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': ['scale', 'auto', 0.1, 1, 10]\n",
        "}\n",
        "\n",
        "# Train an SVM classifier using GridSearchCV\n",
        "svm_classifier = svm.SVC(kernel='rbf')\n",
        "grid_search = GridSearchCV(estimator=svm_classifier, param_grid=param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "\n",
        "# Train the best model on the training set\n",
        "best_model = grid_search.best_estimator_\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "\n",
        "# Explanation\n",
        "# 1. Loading the Wine Dataset: The program loads the Wine dataset using sklearn.datasets.load_wine().\n",
        "# 2. Splitting the Dataset: The dataset is split into training and testing sets using train_test_split().\n",
        "# 3. Defining the Hyperparameter Grid: A grid of hyperparameters (C and gamma) is defined for GridSearchCV.\n",
        "# 4. Training the SVM Classifier: An SVM classifier is trained using GridSearchCV with the defined hyperparameter grid.\n",
        "# 5. Printing Best Hyperparameters: The best hyperparameters are printed using grid_search.best_params_.\n",
        "# 6. Calculating Accuracy: The accuracy of the best model is calculated using accuracy_score() and printed.\n",
        "\n",
        "# This program demonstrates how to use GridSearchCV to find the best hyperparameters for an SVM classifier on the Wine dataset and evaluate its performance."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oz8ell_0L-Zg",
        "outputId": "5386593f-4c74-48b1-ec2e-71ea341ade51"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'C': 100, 'gamma': 'scale'}\n",
            "Accuracy: 0.8333333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "\n",
        "● Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using\n",
        "sklearn.datasets.fetch_20newsgroups).\n",
        "\n",
        "● Print the model's ROC-AUC score for its predictions"
      ],
      "metadata": {
        "id": "8G20OBvdMO6V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ## Overview\n",
        "- ### Task Description: This Python program trains a Naïve Bayes Classifier (MultinomialNB) on the 20 Newsgroups dataset, a multi-class text classification dataset with 20 categories. Text data is vectorized using TF-IDF for feature extraction. The dataset is split into train/test sets, the model is trained, and the ROC-AUC score is computed using one-vs-rest (ovr) for multi-class evaluation.\n",
        "- ### Key Libraries Used:\n",
        " 1. **sklearn.datasets:** To fetch the 20 Newsgroups dataset (subset to 4 categories for efficiency and faster execution).\n",
        " 2. **sklearn.feature_extraction.text:** TfidfVectorizer for converting text to numerical features.\n",
        " 3. **sklearn.naive_bayes:** MultinomialNB classifier.\n",
        " 4. **sklearn.model_selection:** train_test_split for data splitting.\n",
        " 5. **sklearn.metrics:** roc_auc_score for evaluation.\n",
        "- ### Assumptions and Simplifications:\n",
        " 1. Uses a subset of 4 categories (e.g., 'alt.atheism', 'comp.graphics', 'rec.sport.baseball', 'talk.religion.misc') to reduce computational load; full dataset can be used by removing categories.\n",
        " 2. ROC-AUC is computed for multi-class using multi_class='ovr' (one-vs-rest), which averages AUC scores across binary problems.\n",
        " 3. No hyperparameter tuning; default settings for simplicity.\n",
        "- ### Expected Output: The program prints the ROC-AUC score (typically around 0.85-0.95 for this setup).\n",
        "\n",
        "# Execution Notes\n",
        "1. **Running the Code:** Ensure scikit-learn is installed (pip install scikit-learn). The first run downloads the dataset (~10-20 MB).\n",
        "2. **Customization:**\n",
        "   - For the full 20 categories, comment out the categories line and use newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes')).\n",
        "   - Adjust max_features in TfidfVectorizer for more/less vocabulary size.\n",
        "   - For binary classification (simpler ROC-AUC), select only two categories.\n",
        "3. Performance Insights:\n",
        "   - MultinomialNB assumes feature independence and works well on text data.\n",
        "   - TF-IDF helps by weighting important terms and downplaying common words.\n",
        "   - Typical ROC-AUC: 0.90+ on this subset; lower on noisier full data due to class imbalance."
      ],
      "metadata": {
        "id": "H18l1eDzOpEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Fetch the 20 Newsgroups dataset (subset for efficiency)\n",
        "categories = ['alt.atheism', 'comp.graphics', 'rec.sport.baseball', 'talk.religion.misc']\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# Extract text and labels\n",
        "X = newsgroups.data\n",
        "y = newsgroups.target\n",
        "\n",
        "# Step 2: Vectorize the text data using TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')  # Limit features for speed\n",
        "X_vectorized = vectorizer.fit_transform(X)\n",
        "\n",
        "# Step 3: Split the data into train and test sets (80-20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Step 4: Train the Naïve Bayes Classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Predict probabilities on the test set\n",
        "y_proba = nb_classifier.predict_proba(X_test)\n",
        "\n",
        "# Step 6: Compute and print the ROC-AUC score (multi-class: one-vs-rest)\n",
        "roc_auc = roc_auc_score(y_test, y_proba, multi_class='ovr')\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWXF9-ftOeWF",
        "outputId": "2b4c907d-6293-49f1-dd2c-63e953b2869c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9688\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you’re working as a data scientist for a company that handles\n",
        "email communications.\n",
        "\n",
        "Your task is to automatically classify emails as Spam or Not Spam. The emails may\n",
        "contain:\n",
        "\n",
        "● Text with diverse vocabulary\n",
        "\n",
        "● Potential class imbalance (far more legitimate emails than spam)\n",
        "\n",
        "● Some incomplete or missing data\n",
        "\n",
        "Explain the approach you would take to:\n",
        "\n",
        "● Preprocess the data (e.g. text vectorization, handling missing data)\n",
        "\n",
        "● Choose and justify an appropriate model (SVM vs. Naïve Bayes)\n",
        "\n",
        "● Address class imbalance\n",
        "\n",
        "● Evaluate the performance of your solution with suitable metrics\n",
        "And explain the business impact of your solution.\n",
        "- ## Email Classification Approach\n",
        "### Preprocessing the Data\n",
        "1. **Text Vectorization:** Use techniques like TF-IDF (Term Frequency-Inverse Document Frequency) or word embeddings (e.g., Word2Vec, GloVe) to convert text into numerical features.\n",
        "2. **Handling Missing Data:** For missing subject lines or sender information, consider imputing with a special token (e.g., \"Unknown\") or removing those features if they're not crucial.\n",
        "3. **Text Preprocessing:** Apply techniques like tokenization, stopword removal, stemming/lemmatization, and removing special characters to normalize the text data.\n",
        "\n",
        "### Choosing and Justifying a Model\n",
        "1. **Model Comparison:** Both SVM and Naïve Bayes can be effective for text classification tasks.\n",
        "    - **SVM:** SVM is a robust model that can handle high-dimensional data and non-linear relationships. It's suitable for text classification tasks with a large number of features.\n",
        "    - **Naïve Bayes:** Naïve Bayes is a simple and efficient model that performs well for text classification tasks, especially when the features are conditionally independent.\n",
        "2. **Justification:** Considering the potential complexity of the text data and the need for robustness, SVM might be a better choice. However, Naïve Bayes is a good option if the features are relatively independent and the dataset is large.\n",
        "\n",
        "### Addressing Class Imbalance\n",
        "1. **Resampling Techniques:** Use oversampling the minority class (spam emails), undersampling the majority class (legitimate emails), or generating synthetic samples using techniques like SMOTE (Synthetic Minority Over-sampling Technique).\n",
        "2. **Cost-Sensitive Learning:** Assign different costs to misclassification errors, penalizing false negatives (spam emails classified as legitimate) more heavily than false positives (legitimate emails classified as spam).\n",
        "3. **Class Weighting:** Use class weights in the model to give more importance to the minority class.\n",
        "\n",
        "### Evaluating Performance\n",
        "1. **Metrics:** Use metrics like precision, recall, F1-score, and ROC-AUC score to evaluate the model's performance.\n",
        "    - **Precision:** Measures the proportion of true positives (correctly classified spam emails) among all positive predictions.\n",
        "    - **Recall:** Measures the proportion of true positives among all actual spam emails.\n",
        "    - **F1-score:** Harmonic mean of precision and recall.\n",
        "2. **Cross-Validation:** Use techniques like k-fold cross-validation to ensure the model's performance is robust and generalizable.\n",
        "\n",
        "### Business Impact\n",
        "1. **Improved Email Filtering:** An effective email classification solution can significantly reduce the number of spam emails reaching users' inboxes, improving their productivity and reducing the risk of phishing attacks.\n",
        "2. **Enhanced User Experience:** By accurately classifying emails, the solution can help users focus on important emails and reduce the time spent on managing spam.\n",
        "3. **Increased Security:** By detecting and filtering out spam emails, the solution can help prevent phishing attacks, malware distribution, and other cyber threats.\n",
        "4. **Cost Savings:** An effective email classification solution can reduce the costs associated with manual email filtering, IT support, and potential losses due to cyber threats.\n",
        "\n",
        "By implementing an effective email classification solution, the company can improve user experience, enhance security, and reduce costs associated with spam emails."
      ],
      "metadata": {
        "id": "ZOVkgT06MwOi"
      }
    }
  ]
}